"""
Script de verificaci√≥n r√°pida para Ollama.

Verifica que Ollama est√© instalado, funcionando, y puede descargar/usar modelos.
"""

import requests
import json
import sys
from pathlib import Path

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent / "src"))


def check_ollama_installation():
    """Verifica si Ollama est√° instalado y ejecut√°ndose."""
    print("üîç Verificando instalaci√≥n de Ollama...")

    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama est√° ejecut√°ndose correctamente")
            return True
        else:
            print(f"‚ùå Ollama responde con c√≥digo: {response.status_code}")
            return False
    except requests.ConnectionError:
        print("‚ùå No se puede conectar a Ollama")
        print("   ¬øEst√° Ollama ejecut√°ndose? Prueba: ollama serve")
        return False
    except requests.Timeout:
        print("‚ùå Timeout conectando a Ollama")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False


def list_available_models():
    """Lista los modelos disponibles en Ollama."""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])

            if models:
                print(f"\nüì¶ Modelos instalados ({len(models)}):")
                for model in models:
                    name = model.get("name", "unknown")
                    size = model.get("size", 0)
                    size_gb = size / (1024**3) if size > 0 else 0
                    print(f"   ‚Ä¢ {name} ({size_gb:.1f} GB)")
                return models
            else:
                print("\nüì¶ No hay modelos instalados")
                return []
        else:
            print("‚ùå Error obteniendo lista de modelos")
            return []
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return []


def recommend_model():
    """Recomienda un modelo seg√∫n la RAM disponible."""
    try:
        import psutil

        total_ram_gb = psutil.virtual_memory().total / (1024**3)

        print(f"\nüíæ RAM detectada: {total_ram_gb:.1f} GB")

        if total_ram_gb < 8:
            recommended = "llama3.2:3b"
            print(f"üí° Modelo recomendado: {recommended} (~2GB)")
        elif total_ram_gb < 16:
            recommended = "mistral:7b"
            print(f"üí° Modelo recomendado: {recommended} (~4GB)")
        else:
            recommended = "llama3.1:8b"
            print(f"üí° Modelo recomendado: {recommended} (~4.7GB)")

        print(f"\nüì• Para descargar: ollama pull {recommended}")
        return recommended

    except ImportError:
        print("‚ùå No se puede detectar RAM (falta psutil)")
        return "llama3.2:3b"


def test_simple_generation():
    """Prueba una generaci√≥n simple de texto."""
    models = list_available_models()

    if not models:
        print("\n‚ö†Ô∏è  No hay modelos para probar")
        return False

    # Usar el primer modelo disponible
    model_name = models[0].get("name", "")
    print(f"\nüß™ Probando generaci√≥n con {model_name}...")

    try:
        request_data = {
            "model": model_name,
            "prompt": "¬øCu√°l es la capital de Espa√±a?",
            "stream": False,
            "options": {
                "temperature": 0.1,
                "num_predict": 50,
            },
        }

        response = requests.post(
            "http://localhost:11434/api/generate", json=request_data, timeout=60
        )

        if response.status_code == 200:
            data = response.json()
            generated_text = data.get("response", "").strip()
            print(f"‚úÖ Respuesta: {generated_text[:100]}...")
            return True
        else:
            print(f"‚ùå Error en generaci√≥n: {response.status_code}")
            return False

    except Exception as e:
        print(f"‚ùå Error en generaci√≥n: {e}")
        return False


def main():
    print("=" * 60)
    print("üöÄ VERIFICADOR DE OLLAMA")
    print("=" * 60)

    # 1. Verificar instalaci√≥n
    if not check_ollama_installation():
        print("\nüìù Pasos para instalar Ollama:")
        print("   1. Descargar desde: https://ollama.ai/")
        print("   2. Instalar el ejecutable")
        print("   3. Ejecutar: ollama serve")
        print("   4. Ejecutar este script nuevamente")
        return

    # 2. Listar modelos
    models = list_available_models()

    # 3. Recomendar modelo si no hay ninguno
    if not models:
        recommend_model()
        print("\nüìù Despu√©s de descargar un modelo:")
        print("   1. python verificar_ollama.py  # Verificar instalaci√≥n")
        print("   2. python test_modelos.py      # Probar con problemas")
        return

    # 4. Probar generaci√≥n
    if test_simple_generation():
        print("\nüéâ ¬°Ollama est√° funcionando correctamente!")
        print("\nüìù Pr√≥ximos pasos:")
        print("   ‚Ä¢ python test_modelos.py  # Probar con problemas de optimizaci√≥n")
        print(
            "   ‚Ä¢ python nlp_simplex.py --nlp --file ejemplos/nlp/problema_complejo.txt"
        )
    else:
        print("\n‚ö†Ô∏è  Ollama est√° instalado pero hay problemas con la generaci√≥n")


if __name__ == "__main__":
    main()
