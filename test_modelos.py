"""
Test autom√°tico de modelos NLP para resolver el problema complejo.

Este script prueba diferentes modelos en orden de eficiencia hasta encontrar
uno que resuelva correctamente el problema de las 3 plantas.
"""

import sys
import logging
from pathlib import Path
import time

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from nlp import (
    OllamaNLPProcessor,
    NLPModelType,
    ModelSelector,
)


# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


def test_model(model_type: NLPModelType, problem_text: str, timeout: int = 600) -> bool:
    """
    Prueba un modelo espec√≠fico con el problema.

    Returns:
        True si el modelo resolvi√≥ el problema correctamente, False si no.
    """
    logger.info("=" * 70)
    logger.info(f"üß™ Probando modelo: {model_type.value}")
    logger.info("=" * 70)

    try:
        start_time = time.time()

        # Crear procesador Ollama con el modelo espec√≠fico
        processor = OllamaNLPProcessor(
            model_type=model_type,
        )

        # Procesar el problema
        logger.info("üìù Procesando problema...")
        result = processor.process_text(problem_text)

        elapsed_time = time.time() - start_time

        # Verificar resultado
        if result.success:
            logger.info(f"‚úÖ √âXITO! Modelo {model_type.value} resolvi√≥ el problema")
            logger.info(f"‚è±Ô∏è  Tiempo: {elapsed_time:.1f} segundos")
            logger.info(f"üìä Confianza: {result.confidence_score:.2%}")

            if result.problem:
                logger.info(
                    f"üìà Variables: {len(result.problem.objective_coefficients)}"
                )
                logger.info(f"üìã Restricciones: {len(result.problem.constraints)}")
                logger.info(f"üéØ Objetivo: {result.problem.objective_type}")

            return True
        else:
            logger.warning(f"‚ùå Fall√≥: {result.error_message}")
            logger.info(f"‚è±Ô∏è  Tiempo hasta fallo: {elapsed_time:.1f} segundos")
            return False

    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è  Test interrumpido por el usuario")
        raise
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        return False


def main():
    """Ejecuta test con Mistral 7B."""

    print("\n" + "=" * 70)
    print("üî¨ TEST DE MISTRAL 7B")
    print("=" * 70)

    # Detectar memoria RAM disponible
    import psutil
    total_ram_gb = psutil.virtual_memory().total / (1024**3)
    logger.info(f"üñ•Ô∏è  RAM detectada: {total_ram_gb:.1f} GB")

    # Verificar que hay suficiente RAM para Mistral 7B
    if total_ram_gb < 6:
        logger.warning("‚ö†Ô∏è  RAM baja detectada. Mistral 7B requiere al menos 6GB.")
        print("ÔøΩ Considera usar un modelo m√°s ligero con Ollama")
        return

    # Usar √∫nicamente Mistral 7B
    models_to_test = [NLPModelType.MISTRAL_7B]
    logger.info("üéØ Usando modelo predeterminado: Mistral 7B")

    # Cargar problema
    problem_file = Path("ejemplos/nlp/problema_complejo.txt")
    if not problem_file.exists():
        logger.error(f"‚ùå No se encontr√≥ el archivo: {problem_file}")
        return

    problem_text = problem_file.read_text(encoding="utf-8")
    logger.info(f"\nüìÑ Problema cargado: {len(problem_text)} caracteres\n")

    # Mostrar an√°lisis autom√°tico primero
    logger.info("üîç An√°lisis autom√°tico del problema:")
    selector = ModelSelector()
    recommended_model = selector.select_model(problem_text)
    logger.info(f"   Modelo recomendado: {recommended_model.value}\n")

    print("\nüìã Modelos a probar (en orden):")
    for i, model in enumerate(models_to_test, 1):
        print(f"   {i}. {model.value}")
    print()

    # Probar cada modelo
    for i, model_type in enumerate(models_to_test, 1):
        logger.info(f"\n{'='*70}")
        logger.info(f"Test {i}/{len(models_to_test)}")
        logger.info(f"{'='*70}\n")

        try:
            success = test_model(model_type, problem_text)

            if success:
                print("\n" + "=" * 70)
                print("üéâ ¬°PROBLEMA RESUELTO!")
                print("=" * 70)
                print(f"\n‚úÖ Modelo exitoso: {model_type.value}")
                print("\nüí° Puedes usar este modelo en el futuro especificando:")
                print(
                    f"   python nlp_simplex.py --nlp --model {model_type.name.lower()} --file problema.txt"
                )
                print()
                return

        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è  Tests interrumpidos por el usuario")
            return
        except Exception as e:
            logger.error(f"Error inesperado: {e}")
            continue

        # Pausa entre tests
        if i < len(models_to_test):
            logger.info("\n‚è≥ Esperando 5 segundos antes del siguiente test...\n")
            time.sleep(5)

    # Si llegamos aqu√≠, ning√∫n modelo funcion√≥
    print("\n" + "=" * 70)
    print("ÔøΩ AN√ÅLISIS DE RESULTADOS")
    print("=" * 70)
    print("\nÔøΩ Resumen:")
    print(f"   ‚Ä¢ RAM detectada: {total_ram_gb:.1f} GB")
    print(f"   ‚Ä¢ Modelos probados: {len(models_to_test)}")
    print(f"   ‚Ä¢ Problema: {len(problem_text)} caracteres")
    
    print("\n‚ùå Problemas encontrados:")
    print("   ‚Ä¢ Los modelos FLAN-T5 no generan JSON estructurado correctamente")
    print("   ‚Ä¢ Los modelos m√°s potentes (Phi-3, Gemma) requieren dependencias complejas")
    print("   ‚Ä¢ El problema es complejo (9 variables, m√∫ltiples restricciones)")
    
    print("\n‚úÖ Pasos siguientes:")
    print("   1. ‚úì Ollama est√° instalado o instal√°ndose")
    print("   2. üì• Descargar modelo recomendado:")
    if total_ram_gb < 8:
        print("      ollama pull llama3.2:3b")
    elif total_ram_gb < 16:
        print("      ollama pull mistral:7b")
    else:
        print("      ollama pull llama3.1:8b")
    print("   3. üîÑ Ejecutar este script nuevamente")
    print("   4. üß™ Probar con: python nlp_simplex.py --nlp --file problema.txt")
    
    print("\nüí° Comandos √∫tiles de Ollama:")
    print("   ‚Ä¢ ollama list          # Ver modelos descargados")
    print("   ‚Ä¢ ollama serve         # Iniciar servidor (autom√°tico)")  
    print("   ‚Ä¢ ollama pull <model>  # Descargar modelo")
    print()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Test cancelado por el usuario")
    except Exception as e:
        logger.error(f"\n‚ùå Error fatal: {e}")
        sys.exit(1)
