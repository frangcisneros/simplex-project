# Modelos NLP Disponibles

## ¬øPor qu√© no funciona con problemas complejos?

Los modelos **FLAN-T5** (small/base) son muy ligeros y r√°pidos, pero tienen **precisi√≥n limitada** para problemas complejos con muchas variables y restricciones. Son buenos para problemas simples (2-3 variables, pocas restricciones).

Para el **problema de las 3 plantas** (9 variables, 9 restricciones) necesitas modelos m√°s potentes.

---

## Modelos Disponibles

### 1. Modelos T5 (Ligeros - Baja/Media Precisi√≥n)

| Modelo            | Tama√±o | RAM | GPU | Velocidad     | Precisi√≥n         | Uso Recomendado       |
| ----------------- | ------ | --- | --- | ------------- | ----------------- | --------------------- |
| **FLAN-T5-small** | 80MB   | 2GB | No  | ‚ö° Muy r√°pido | ‚≠ê‚≠ê Baja         | Problemas muy simples |
| **FLAN-T5-base**  | 250MB  | 4GB | No  | ‚ö° R√°pido     | ‚≠ê‚≠ê‚≠ê Media-baja | Problemas simples     |
| **FLAN-T5-large** | 780MB  | 6GB | No  | üêå Lento      | ‚≠ê‚≠ê‚≠ê Media      | Problemas medios      |

**‚úÖ Ventajas:** R√°pidos, funcionan en CPU, no requieren GPU  
**‚ùå Desventajas:** Baja precisi√≥n en problemas complejos  
**Resultado con problema_complejo.txt:** ‚ùå No funciona

---

### 2. Modelos Peque√±os Potentes (Nueva Generaci√≥n)

| Modelo         | Tama√±o | RAM  | GPU      | Velocidad | Precisi√≥n           | Uso Recomendado                       |
| -------------- | ------ | ---- | -------- | --------- | ------------------- | ------------------------------------- |
| **Phi-3-mini** | 3.8GB  | 6GB  | Opcional | ‚ö° R√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê Alta       | **¬°RECOMENDADO!** Problemas complejos |
| **Gemma-2B**   | 2GB    | 4GB  | Opcional | ‚ö° R√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê Alta       | Problemas medios-complejos            |
| **Gemma-7B**   | 7GB    | 10GB | S√≠       | üêå Medio  | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy alta | Problemas muy complejos               |

**‚úÖ Ventajas:** Excelente balance tama√±o/precisi√≥n, funcionan en CPU (m√°s lento)  
**‚ùå Desventajas:** M√°s lentos que T5, requieren m√°s RAM  
**Resultado con problema_complejo.txt:** ‚úÖ Probablemente funcione bien (especialmente Phi-3)

**üéØ MEJOR OPCI√ìN PARA TI:** **Phi-3-mini**

- Tu sistema: 11.9GB RAM, sin GPU ‚Üí Capacidad MEDIUM
- Phi-3 es peque√±o (3.8GB) pero muy preciso
- Funciona en CPU (ser√° lento pero funcionar√°)
- Es de Microsoft, muy bien optimizado

---

### 3. Modelos Grandes (M√°xima Precisi√≥n)

| Modelo         | Tama√±o | RAM  | GPU      | Velocidad | Precisi√≥n           | Uso Recomendado         |
| -------------- | ------ | ---- | -------- | --------- | ------------------- | ----------------------- |
| **Mistral-7B** | 7GB    | 16GB | S√≠ (8GB) | üêå Lento  | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy alta | Problemas muy complejos |
| **Llama-3-8B** | 8GB    | 16GB | S√≠ (8GB) | üêå Lento  | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy alta | Problemas muy complejos |

**‚úÖ Ventajas:** M√°xima precisi√≥n, pueden resolver cualquier problema  
**‚ùå Desventajas:** Requieren GPU potente, lentos, ocupan mucho espacio  
**Resultado con problema_complejo.txt:** ‚úÖ‚úÖ Funcionar√≠an perfectamente  
**‚ö†Ô∏è Para tu sistema:** No recomendados (requieren GPU, t√∫ no tienes)

---

### 4. APIs (Alternativa Sin Descargar)

| Servicio           | Costo     | Velocidad     | Precisi√≥n         | Configuraci√≥n   |
| ------------------ | --------- | ------------- | ----------------- | --------------- |
| **OpenAI GPT-4**   | üí∞ Pago   | ‚ö° R√°pido     | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê M√°xima | API key         |
| **OpenAI GPT-3.5** | üí∞ Barato | ‚ö° Muy r√°pido | ‚≠ê‚≠ê‚≠ê‚≠ê Alta     | API key         |
| **Ollama (local)** | üíö Gratis | üêå Variable   | ‚≠ê‚≠ê‚≠ê‚≠ê Alta     | Instalar Ollama |

**‚úÖ Ventajas:** No descargar modelos, siempre la √∫ltima versi√≥n, muy precisos  
**‚ùå Desventajas:** Requieren internet, algunos son de pago  
**Resultado con problema_complejo.txt:** ‚úÖ‚úÖ Funcionar√≠an perfectamente

**Ollama es GRATIS y local:**

```bash
# Instalar Ollama (https://ollama.ai)
ollama pull llama3
ollama pull mistral

# Luego el sistema puede usarlo autom√°ticamente
```

---

## ¬øQu√© Modelo Usar Para Tu Caso?

### Tu Sistema Actual:

- RAM: 11.9GB ‚úÖ
- GPU: No ‚ùå
- Capacidad: MEDIUM

### Recomendaciones por Prioridad:

#### ü•á **Opci√≥n 1: Phi-3-mini** (RECOMENDADO)

```bash
# No requiere configuraci√≥n adicional
# El sistema lo descargar√° autom√°ticamente
python nlp_simplex.py --nlp --file ejemplos/nlp/problema_complejo.txt
```

**Pros:** Mejor balance precisi√≥n/tama√±o, funcionar√° en tu CPU  
**Cons:** Primera vez tardar√° en descargar (3.8GB), procesamiento ~2-5 minutos

#### ü•à **Opci√≥n 2: Ollama + Llama3** (Gratis, muy preciso)

```bash
# 1. Instalar Ollama desde https://ollama.ai
# 2. Descargar modelo
ollama pull llama3

# 3. Configurar el sistema para usar Ollama
# (Necesitaremos agregar soporte para Ollama - pr√≥ximo paso)
```

**Pros:** Muy preciso, gratis, actualizaciones autom√°ticas  
**Cons:** Requiere instalar software adicional

#### ü•â **Opci√≥n 3: OpenAI GPT-3.5** (Pago pero preciso)

```bash
# 1. Obtener API key de OpenAI
# 2. Configurar
export OPENAI_API_KEY="tu-key-aqui"

# 3. Usar
# (Necesitaremos agregar soporte para OpenAI - pr√≥ximo paso)
```

**Pros:** Muy preciso, r√°pido, no usa tu computadora  
**Cons:** De pago (~$0.50 por 1000 solicitudes)

---

## C√≥mo Forzar un Modelo Espec√≠fico

### M√©todo 1: Por l√≠nea de comandos

```bash
# Pr√≥ximamente agregaremos:
python nlp_simplex.py --nlp --model phi-3-mini --file problema.txt
```

### M√©todo 2: En c√≥digo Python

```python
from src.nlp import TransformerNLPProcessor, NLPModelType

# Forzar Phi-3
processor = TransformerNLPProcessor(
    model_type=NLPModelType.PHI_3_MINI,
    auto_select_model=False  # Desactivar selecci√≥n autom√°tica
)

result = processor.process_text(problem_text)
```

---

## Comparaci√≥n de Resultados Esperados

| Problema                  | T5-small | T5-base | Phi-3 | Mistral | GPT-4 |
| ------------------------- | -------- | ------- | ----- | ------- | ----- |
| Simple (2 vars)           | ‚úÖ       | ‚úÖ      | ‚úÖ    | ‚úÖ      | ‚úÖ    |
| Medio (4-6 vars)          | ‚ö†Ô∏è       | ‚ö†Ô∏è      | ‚úÖ    | ‚úÖ      | ‚úÖ    |
| Complejo (9+ vars)        | ‚ùå       | ‚ùå      | ‚úÖ    | ‚úÖ      | ‚úÖ    |
| **problema_complejo.txt** | ‚ùå       | ‚ùå      | ‚úÖ    | ‚úÖ      | ‚úÖ    |

---

## Pr√≥ximos Pasos

Para resolver tu problema actualmente:

### Opci√≥n A: Usar Phi-3-mini (Recomendado)

Te voy a ayudar a configurar el sistema para que use Phi-3-mini por defecto para problemas complejos.

### Opci√≥n B: Integrar Ollama

Puedo agregar soporte para Ollama, que es gratis y muy preciso.

### Opci√≥n C: Integrar OpenAI API

Puedo agregar soporte para GPT-3.5/GPT-4 si tienes una API key.

### Opci√≥n D: M√©todo de Respaldo basado en Reglas

Puedo implementar un sistema de extracci√≥n basado en patrones regex como fallback cuando los modelos fallan.

**¬øCu√°l prefieres que implemente primero?**
