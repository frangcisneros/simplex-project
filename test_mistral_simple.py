"""
Test rÃ¡pido solo con Mistral para verificar que el prompt funciona.
"""

import sys
from pathlib import Path

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from nlp import OllamaNLPProcessor, NLPModelType


def test_mistral_only():
    print("ğŸ§ª Probando solo Mistral con prompt mejorado...")

    # Cargar problema
    problem_file = Path("ejemplos/nlp/problema_complejo.txt")
    if not problem_file.exists():
        print("âŒ No se encontrÃ³ el archivo del problema")
        return

    problem_text = problem_file.read_text(encoding="utf-8")
    print(f"ğŸ“„ Problema: {len(problem_text)} caracteres")

    # Crear procesador
    processor = OllamaNLPProcessor(model_type=NLPModelType.MISTRAL_7B)

    # Verificar disponibilidad
    if not processor.is_available():
        print("âŒ Mistral no estÃ¡ disponible")
        return

    print("âœ… Mistral estÃ¡ disponible")
    print("ğŸ”„ Generando respuesta...")

    # Procesar
    result = processor.process_text(problem_text)

    if result.success:
        print("ğŸ‰ Â¡Ã‰XITO!")
        print(f"ğŸ“Š Confianza: {result.confidence_score:.2%}")
        if result.problem:
            print(f"ğŸ¯ Objetivo: {result.problem.objective_type}")
            print(f"ğŸ“ˆ Variables: {len(result.problem.objective_coefficients)}")
            print(f"ğŸ“‹ Restricciones: {len(result.problem.constraints)}")
            print(f"ğŸ·ï¸  Nombres: {result.problem.variable_names}")
    else:
        print("âŒ FallÃ³:")
        print(f"   {result.error_message}")


if __name__ == "__main__":
    test_mistral_only()
